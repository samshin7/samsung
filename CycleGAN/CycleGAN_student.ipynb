{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN, Image-to-Image Translation / 2019. 4. 25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Load and Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in and transforming data\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# visualizing data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(image_type, image_dir='horse2zebra', \n",
    "                    image_size=128, batch_size=16, num_workers=0):\n",
    "    \n",
    "    transform = transforms.Compose([transforms.Resize(image_size), # # resize(to 128x128) and normalize \n",
    "                                    transforms.ToTensor()])\n",
    "\n",
    "    # get training and test directories\n",
    "    image_path = './' + image_dir\n",
    "    train_path = os.path.join(image_path, image_type)\n",
    "    test_path = os.path.join(image_path, 'test_{}'.format(image_type))\n",
    "\n",
    "    # define datasets using ImageFolder\n",
    "    train_dataset = datasets.ImageFolder(train_path, transform)\n",
    "    test_dataset = datasets.ImageFolder(test_path, transform)\n",
    "\n",
    "    # create and return DataLoaders\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test dataloaders for images from the two domains X and Y\n",
    "# image_type = directory names for our data\n",
    "dataloader_X, test_dataloader_X = get_data_loader(image_type='horse')\n",
    "dataloader_Y, test_dataloader_Y = get_data_loader(image_type='zebra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    \n",
    "dataiter = iter(dataloader_X) #반복되는 객체\n",
    "images, _ = dataiter.next()  #first batch # the \"_\" is a placeholder for no labels\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))  \n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "dataiter = iter(dataloader_Y)\n",
    "images, _ = dataiter.next()\n",
    "# show images\n",
    "fig = plt.figure(figsize=(12, 8))  \n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current range\n",
    "img = images[0]\n",
    "\n",
    "print('Min: ', img.min())\n",
    "print('Max: ', img.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper scale function\n",
    "def scale(x, feature_range=(-1, 1)):\n",
    "    ''' Scale takes in an image x and returns that image, scaled\n",
    "       with a feature_range of pixel values from -1 to 1. \n",
    "       This function assumes that the input x is already scaled from 0-255.'''\n",
    "    \n",
    "    # scale from 0-1 to feature_range\n",
    "    min, max = feature_range\n",
    "    x = x * (max - min) + min\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled range\n",
    "scaled_img = scale(img)\n",
    "\n",
    "print('Scaled min: ', scaled_img.min())\n",
    "print('Scaled max: ', scaled_img.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Define the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) function for Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# helper conv function\n",
    "def conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n",
    "    \"\"\"Creates a convolutional layer, with optional batch normalization.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                           kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "    \n",
    "    layers.append(conv_layer)\n",
    "\n",
    "    if batch_norm:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, conv_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Define all convolutional layers\n",
    "        # Should accept an RGB image as input and output a single value\n",
    "\n",
    "        # Convolutional layers, increasing in depth\n",
    "    \n",
    "        #input (16,3,128,128)\n",
    "        self.conv1 =                              \n",
    "        self.conv2 =                               \n",
    "        self.conv3 =                               \n",
    "        self.conv4 =                               \n",
    "        \n",
    "        # Classification layer(batchnorm=false)\n",
    "        self.conv5 = \n",
    "\n",
    "    def forward(self, x):\n",
    "        # relu applied to all conv layers but last\n",
    "        out = F.relu(self.conv1(x))      #결과 size [16, 64, 64, 64]\n",
    "        out = F.relu(self.conv2(out))    #[16, 128, 32, 32]\n",
    "        out = F.relu(self.conv3(out))    #[16, 256, 16, 16]\n",
    "        out = F.relu(self.conv4(out))    #[16, 512, 8, 8]\n",
    "        out = self.conv5(out)            #[16, 1, 7, 7]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) functions/class of Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual block class\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # conv_dim = number of inputs\n",
    "        \n",
    "        # define two convolutional layers + batch normalization that will act as our residual function, F(x)\n",
    "        # layers should have the same shape input as output; I suggest a kernel_size of 3\n",
    "        \n",
    "        self.conv_layer1 = conv(in_channels=conv_dim, out_channels=conv_dim, \n",
    "                                kernel_size=3, stride=1, padding=1, batch_norm=True)\n",
    "        \n",
    "        self.conv_layer2 = conv(in_channels=conv_dim, out_channels=conv_dim, \n",
    "                               kernel_size=3, stride=1, padding=1, batch_norm=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        \n",
    "        \n",
    "        return \n",
    "            \n",
    "def deconv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n",
    "    \"\"\"Creates a transpose convolutional layer, with optional batch normalization.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Generator Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self, conv_dim=64, n_res_blocks=6):\n",
    "        super(CycleGenerator, self).__init__()\n",
    "\n",
    "        # Encoder \n",
    "        \n",
    "        self.conv1 = conv(3, conv_dim, 4)\n",
    "        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n",
    "        self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n",
    "\n",
    "        # Resnet part\n",
    "        res_layers = []\n",
    "\n",
    "        \n",
    "        self.res_blocks = \n",
    "\n",
    "        \n",
    "        \n",
    "        # 3. Decoder\n",
    "        self.deconv1 = deconv(conv_dim*4, conv_dim*2, 4)\n",
    "        self.deconv2 = deconv(conv_dim*2, conv_dim, 4)\n",
    "        self.deconv3 = deconv(conv_dim, 3, 4, batch_norm=False)  # no batch norm on last layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Given an image x, returns a transformed image.\"\"\"\n",
    "        # define feedforward behavior, applying activations as necessary\n",
    "\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.relu(self.conv3(out))\n",
    "\n",
    "        out = self.res_blocks(out)\n",
    "\n",
    "        out = F.relu(self.deconv1(out))\n",
    "        out = F.relu(self.deconv2(out))\n",
    "        out = F.tanh(self.deconv3(out))           # tanh applied to last layer\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(g_conv_dim=64, d_conv_dim=64, n_res_blocks=6):\n",
    "    \n",
    "    G_XtoY = \n",
    "    G_YtoX = \n",
    "    D_X = \n",
    "    D_Y = \n",
    "\n",
    "    if torch.cuda.is_available():   # move models to GPU, if available\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        G_XtoY.to(device)\n",
    "        G_YtoX.to(device)\n",
    "        D_X.to(device)\n",
    "        D_Y.to(device)\n",
    "        print('Models moved to GPU.')\n",
    "    else:\n",
    "        print('Only CPU available.')\n",
    "\n",
    "    return G_XtoY, G_YtoX, D_X, D_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_XtoY, G_YtoX, D_X, D_Y =              # call the function to get models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_mse_loss(D_out):\n",
    "    \n",
    "    return \n",
    "\n",
    "def fake_mse_loss(D_out):\n",
    "    \n",
    "    return \n",
    "\n",
    "def cycle_consistency_loss(real_im, reconstructed_im, lambda_weight):\n",
    "    \n",
    "    \n",
    "    return     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# hyperparams for Adam optimizer #gradient의 지수평균 이용\n",
    "lr=0.0002\n",
    "beta1=0.5\n",
    "beta2=0.999 # default value\n",
    "\n",
    "d_x_optimizer = optim.Adam(D_X.parameters(), lr, [beta1, beta2])\n",
    "d_y_optimizer = \n",
    "\n",
    "g_params = list(G_XtoY.parameters()) + list(G_YtoX.parameters()) \n",
    "g_optimizer = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from other code\n",
    "from help import save_samples, checkpoint\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "\n",
    "def training_loop(dataloader_X, dataloader_Y, test_dataloader_X, test_dataloader_Y, \n",
    "                  n_epochs=1000):\n",
    "    \n",
    "    since = time.time()\n",
    "    print_every=100\n",
    "    \n",
    "    losses = []      # keep track of losses over time\n",
    "    test_iter_X = iter(test_dataloader_X)\n",
    "    test_iter_Y = iter(test_dataloader_Y)\n",
    "\n",
    "    fixed_X = test_iter_X.next()[0]      # Get some fixed data from domains X and Y for sampling. These are images that are held\n",
    "    fixed_Y = test_iter_Y.next()[0]          # constant throughout training, that allow us to inspect the model's performance.\n",
    "    fixed_X = scale(fixed_X) # make sure to scale to a range -1 to 1\n",
    "    fixed_Y = scale(fixed_Y)\n",
    "\n",
    "    iter_X =      \n",
    "    iter_Y = \n",
    "    num_min_cycle= \n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "\n",
    "        if  == 0:         \n",
    "    \n",
    "    \n",
    "    \n",
    "        images_X\n",
    "        images_Y\n",
    "        \n",
    "        \n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        images_X = images_X.to(device)\n",
    "        images_Y = images_Y.to(device)\n",
    "\n",
    "        # ============================================\n",
    "        #            TRAIN THE DISCRIMINATORS\n",
    "        # ============================================\n",
    "\n",
    "        # 1. D_X, real and fake loss components   ##\n",
    "\n",
    "        \n",
    "        D_X_real_loss = \n",
    "\n",
    "        D_X_fake_loss = \n",
    "        \n",
    "        d_x_loss = \n",
    "        \n",
    "        d_x_loss.backward()\n",
    "        d_x_optimizer.step()\n",
    "\n",
    "        \n",
    "        # 2. Second: D_Y, real and fake loss components   ##\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    " \n",
    "        # =========================================\n",
    "        #            TRAIN THE GENERATORS\n",
    "        # =========================================\n",
    "\n",
    "        \n",
    "      \n",
    "      \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Print the log info\n",
    "        if epoch % print_every == 0:\n",
    "            # append real and fake discriminator losses and the generator loss\n",
    "            losses.append((d_x_loss.item(), d_y_loss.item(), g_total_loss.item()))\n",
    "            time_elapsed = time.time() - since\n",
    "            print('Epoch [{:5d}/{:5d}] | d_X_loss: {:6.4f} | d_Y_loss: {:6.4f} | g_total_loss: {:6.4f} time : {:.0f}m {:.0f}s'.format(\n",
    "                    epoch, n_epochs, d_x_loss.item(), d_y_loss.item(), g_total_loss.item(),time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "\n",
    "        sample_every=200\n",
    "        # Save the generated samples\n",
    "        if epoch % sample_every == 0:\n",
    "            G_YtoX.eval() # set generators to eval mode for sample generation\n",
    "            G_XtoY.eval()\n",
    "            save_samples(epoch, fixed_Y, fixed_X, G_YtoX, G_XtoY, batch_size=16, sample_dir='samples_cyclegan')\n",
    "            G_YtoX.train()\n",
    "            G_XtoY.train()\n",
    "\n",
    "\n",
    "                    # uncomment these lines, if you want to save your model\n",
    "#         checkpoint_every=1000\n",
    "#         # Save the model parameters\n",
    "#         if epoch % checkpoint_every == 0:\n",
    "#             checkpoint(epoch, G_XtoY, G_YtoX, D_X, D_Y)\n",
    "\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 6000 # keep this small when testing if a model first works\n",
    "\n",
    "losses = \n",
    "\n",
    "torch.save(D_X.state_dict(),'D_X.pt')\n",
    "torch.save(D_Y.state_dict(),'D_Y.pt')\n",
    "torch.save(G_XtoY.state_dict(),'D_XtoY.pt')\n",
    "torch.save(G_YtoX.state_dict(),'D_YtoX.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluate the Result with Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "# helper visualization code\n",
    "def view_samples(iteration, sample_dir='samples_cyclegan'):\n",
    "    \n",
    "    # samples are named by iteration\n",
    "    path_XtoY = os.path.join(sample_dir, 'sample-{:06d}-X-Y.png'.format(iteration))\n",
    "    path_YtoX = os.path.join(sample_dir, 'sample-{:06d}-Y-X.png'.format(iteration))\n",
    "    \n",
    "    # read in those samples\n",
    "    try: \n",
    "        x2y = mpimg.imread(path_XtoY)\n",
    "        y2x = mpimg.imread(path_YtoX)\n",
    "    except:\n",
    "        print('Invalid number of iterations.')\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(18,20), nrows=2, ncols=1, sharey=True, sharex=True)\n",
    "    ax1.imshow(x2y)\n",
    "    ax1.set_title('X to Y')\n",
    "    ax2.imshow(y2x)\n",
    "    ax2.set_title('Y to X')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view samples at iteration 100\n",
    "view_samples(100, 'samples_cyclegan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view samples at iteration 4000\n",
    "view_samples(4000, 'samples_cyclegan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "reference : https://github.com/udacity/deep-learning-v2-pytorch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
